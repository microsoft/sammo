{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61643fe1-aa70-4f99-a3c4-a4d0577b70f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Load from parent directory if not installed\n",
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"sammo\"):\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"../../\")\n",
    "\n",
    "CACHE_FILE = \"cache/custom_runners.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65550492-591f-44f3-85a1-9bee6db2316e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sammo\n",
    "import getpass\n",
    "\n",
    "from sammo.components import GenerateText, Output\n",
    "from sammo.utils import serialize_json\n",
    "\n",
    "from sammo.base import LLMResult, Costs\n",
    "from sammo.runners import BaseRunner\n",
    "\n",
    "_ = sammo.setup_logger(\"WARNING\")  # we're only interested in warnings for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6833909-fd5c-4b2c-b1aa-601ebe1eeb18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Custom Runners\n",
    "\n",
    "To call different backends, `SAMMO` supports custom implementations via {class}`~sammo.base.Runner`. If you have a REST API that you'd like to call, the simplest way to do it is to inherit from {class}`~sammo.runners.BaseRunner~`.\n",
    "\n",
    "In this tutorial, we will write a custom runner to generate text via a DeepInfra endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8752663a-a4c2-4321-8655-75353dfe72d4",
   "metadata": {
    "editable": true,
    "mystnb": {
     "number_source_lines": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sammo.components import GenerateText, Output\n",
    "from sammo.utils import serialize_json\n",
    "\n",
    "from sammo.base import LLMResult, Costs\n",
    "from sammo.runners import BaseRunner\n",
    "\n",
    "\n",
    "class DeepInfraChat(BaseRunner):\n",
    "    async def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_tokens: int | None = None,\n",
    "        randomness: float | None = 0,\n",
    "        seed: int = 0,\n",
    "        priority: int = 0,\n",
    "        **kwargs,\n",
    "    ) -> LLMResult:\n",
    "        formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "        request = dict(\n",
    "            input=formatted_prompt,\n",
    "            max_new_tokens=self._max_context_window or max_tokens,\n",
    "            temperature=randomness,\n",
    "        )\n",
    "        fingerprint = serialize_json({\"seed\": seed, \"generative_model_id\": self._model_id, **request})\n",
    "        return await self._execute_request(request, fingerprint, priority)\n",
    "\n",
    "    async def _call_backend(self, request: dict) -> dict:\n",
    "        async with self._get_session() as session:\n",
    "            async with session.post(\n",
    "                f\"https://api.deepinfra.com/v1/inference/{self._model_id}\",\n",
    "                json=request,\n",
    "                headers={\"Authorization\": f\"Bearer {self._api_config['api_key']}\"}\n",
    "            ) as response:\n",
    "                return await response.json()\n",
    "\n",
    "    def _to_llm_result(self, request: dict, json_data: dict, fingerprint: str | bytes):\n",
    "        return LLMResult(\n",
    "            json_data[\"results\"][0][\"generated_text\"],\n",
    "            costs=Costs(json_data[\"num_input_tokens\"], json_data[\"num_tokens\"]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4504c53-757b-4f83-8e08-a584ad85e14c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your API key ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------------------------------------+\n",
      "| input   | output                                                      |\n",
      "+=========+=============================================================+\n",
      "| None    | Horses, majestic creatures, have accompanied humans for     |\n",
      "|         | thousands of years, serving in transportation, agriculture, |\n",
      "|         | and warfare. Today, they are cherished for companionship,   |\n",
      "|         | sport, and therapy. With their powerful build, graceful     |\n",
      "|         | movements, and intuitive nature, horses continue to inspire |\n",
      "|         | and connect us to the natural world. Their enduring bond    |\n",
      "|         | with humans is a testament to their intelligence and        |\n",
      "|         | emotional depth.                                            |\n",
      "+---------+-------------------------------------------------------------+\n",
      "Constants: None\n"
     ]
    }
   ],
   "source": [
    "runner = DeepInfraChat(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\", api_config={\"api_key\": getpass.getpass(\"Enter your API key\")}\n",
    ")\n",
    "print(Output(GenerateText(\"Generate a 50 word essay about horses.\")).run(runner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43b229-ba94-4b24-81df-66456f84f442",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The three things we had to implement were\n",
    "\n",
    "1. `generate_text()`: To format the prompt into a dictionary and compute a fingerprint for \n",
    "2. `_call_backend()`: To make the actual REST request\n",
    "3. `_to_llm_result()`: To convert the JSON object into an LLM result instance.\n",
    "\n",
    "That's it! The parent class will take care of all caching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
